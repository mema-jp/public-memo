在 `lazy-apps=true` 模式下，每个worker都是独立启动应用的，这种情况下预热更需要策略。

## lazy-apps=true 的特点

- 每个worker独立加载应用代码
- 不共享内存（适合避免某些库的fork问题）
- **必须确保每个worker都收到至少一个请求才能完成初始化**

## 推荐方案：并发预热所有worker

### 方案1：使用并发请求强制触发所有worker

```python
# warmup.py
import requests
import concurrent.futures
import time
from collections import defaultdict

def warmup_lazy_workers():
    base_url = "http://localhost:8000"
    worker_count = 5  # 与uwsgi.ini中的processes数量一致
    
    print("等待uWSGI启动...")
    wait_for_service(base_url)
    
    print(f"\n开始并发预热 {worker_count} 个workers...")
    
    # 方法1: 并发请求，确保同时触发多个worker
    workers_seen = set()
    
    def send_warmup_request(request_id):
        """发送预热请求并返回worker信息"""
        try:
            resp = requests.post(
                f"{base_url}/predict",
                json={"warmup": True, "request_id": request_id},
                timeout=60,  # lazy模式首次加载可能需要更长时间
                headers={"X-Request-ID": str(request_id)}
            )
            
            worker_info = resp.json().get('worker_pid') or resp.headers.get('X-Worker-PID')
            return {
                'success': True,
                'worker': worker_info,
                'status': resp.status_code,
                'time': resp.elapsed.total_seconds()
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'request_id': request_id
            }
    
    # 第一轮：并发请求，触发所有worker
    print(f"第一轮: 发送 {worker_count} 个并发请求...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=worker_count) as executor:
        futures = [executor.submit(send_warmup_request, i) for i in range(worker_count)]
        results = [f.result() for f in concurrent.futures.as_completed(futures)]
    
    # 统计已触发的worker
    for result in results:
        if result['success'] and result['worker']:
            workers_seen.add(result['worker'])
            print(f"✓ Worker {result['worker']} 已预热 (耗时: {result.get('time', 0):.2f}s)")
    
    print(f"\n第一轮完成: {len(workers_seen)}/{worker_count} 个workers已预热")
    
    # 第二轮：如果还有worker未被触发，继续发送请求
    if len(workers_seen) < worker_count:
        print(f"\n第二轮: 继续预热剩余的workers...")
        max_additional_attempts = worker_count * 3
        
        for i in range(max_additional_attempts):
            result = send_warmup_request(worker_count + i)
            if result['success'] and result['worker']:
                if result['worker'] not in workers_seen:
                    workers_seen.add(result['worker'])
                    print(f"✓ Worker {result['worker']} 已预热 ({len(workers_seen)}/{worker_count})")
                    
                if len(workers_seen) >= worker_count:
                    break
            
            time.sleep(0.1)
    
    # 最终报告
    print(f"\n{'='*50}")
    if len(workers_seen) >= worker_count:
        print(f"✓ 预热成功: 所有 {worker_count} 个workers已就绪")
        print(f"已预热的worker PIDs: {sorted(workers_seen)}")
    else:
        print(f"⚠ 警告: 只预热了 {len(workers_seen)}/{worker_count} 个workers")
        print(f"已预热的worker PIDs: {sorted(workers_seen)}")
    
    # 第三轮：功能性预热（可选）
    print(f"\n执行功能性预热...")
    functional_warmup(base_url, worker_count)

def functional_warmup(base_url, rounds=3):
    """对各个端点进行功能性预热"""
    endpoints = [
        {"method": "POST", "path": "/predict", "json": {"data": "test"}},
        {"method": "GET", "path": "/health"},
    ]
    
    for round_num in range(rounds):
        for endpoint in endpoints:
            try:
                resp = requests.request(
                    endpoint["method"],
                    f"{base_url}{endpoint['path']}",
                    json=endpoint.get("json"),
                    timeout=10
                )
                status = "✓" if resp.status_code == 200 else "✗"
                print(f"{status} Round {round_num+1}: {endpoint['method']} {endpoint['path']} -> {resp.status_code}")
            except Exception as e:
                print(f"✗ Round {round_num+1}: {endpoint['method']} {endpoint['path']} -> {e}")
            time.sleep(0.05)

def wait_for_service(base_url, max_retry=60):
    """等待服务启动，lazy模式可能需要更长时间"""
    for i in range(max_retry):
        try:
            resp = requests.get(f"{base_url}/health", timeout=2)
            if resp.status_code == 200:
                print("✓ 服务已响应")
                return True
        except:
            print(f"等待服务启动... ({i+1}/{max_retry})")
            time.sleep(1)
    raise Exception("服务启动超时")

if __name__ == "__main__":
    warmup_lazy_workers()
```

### 方案2：修改应用代码返回worker信息

为了让预热脚本能准确追踪worker，需要在应用中返回worker PID：

**app.py**:

```python
import os
import time
from flask import Flask, jsonify, request, g

app = Flask(__name__)

# 记录worker初始化时间
WORKER_START_TIME = time.time()
WORKER_PID = os.getpid()

# 用于lazy模式的延迟加载
model = None

def get_model():
    """延迟加载模型（lazy-apps模式）"""
    global model
    if model is None:
        print(f"Worker {WORKER_PID} 开始加载模型...")
        start = time.time()
        
        # 你的模型加载逻辑
        model = load_your_ml_model()
        
        load_time = time.time() - start
        print(f"Worker {WORKER_PID} 模型加载完成，耗时: {load_time:.2f}s")
    
    return model

@app.before_request
def before_request():
    """记录请求开始时间"""
    g.start_time = time.time()

@app.after_request
def after_request(response):
    """添加worker信息到响应头"""
    response.headers['X-Worker-PID'] = str(WORKER_PID)
    
    # 添加请求处理时间
    if hasattr(g, 'start_time'):
        elapsed = time.time() - g.start_time
        response.headers['X-Response-Time'] = f"{elapsed:.3f}"
    
    return response

@app.route('/health')
def health():
    """健康检查"""
    return jsonify({
        "status": "ok",
        "worker_pid": WORKER_PID,
        "worker_uptime": time.time() - WORKER_START_TIME,
        "model_loaded": model is not None
    })

@app.route('/predict', methods=['POST'])
def predict():
    """预测接口"""
    is_warmup = request.json.get('warmup', False)
    
    # 触发模型加载
    m = get_model()
    
    if is_warmup:
        return jsonify({
            "status": "warmup_complete",
            "worker_pid": WORKER_PID,
            "message": "Worker warmed up successfully"
        })
    
    # 实际预测逻辑
    result = m.predict(request.json)
    
    return jsonify({
        "result": result,
        "worker_pid": WORKER_PID
    })

@app.route('/worker-info')
def worker_info():
    """Worker信息"""
    return jsonify({
        "pid": WORKER_PID,
        "uptime": time.time() - WORKER_START_TIME,
        "model_loaded": model is not None,
        "model_type": type(model).__name__ if model else None
    })
```

### 方案3：使用uWSGI的stats server监控

**uwsgi.ini**:

```ini
[uwsgi]
http = :8000
chdir = /app
module = app:application
master = true
processes = 5
threads = 2

# lazy模式
lazy-apps = true

# 启用stats服务器（用于监控worker状态）
stats = 127.0.0.1:9191
stats-http = true

# 添加worker信息到响应头
add-header = X-Worker-PID: %(_worker_id)
```

**warmup_with_stats.py**:

```python
import requests
import json
import time
import concurrent.futures

def get_uwsgi_stats():
    """从uWSGI stats服务器获取worker信息"""
    try:
        resp = requests.get("http://127.0.0.1:9191", timeout=2)
        stats = resp.json()
        return stats.get('workers', [])
    except:
        return []

def warmup_with_monitoring():
    base_url = "http://localhost:8000"
    
    # 等待服务启动
    wait_for_service(base_url)
    
    # 获取worker列表
    workers = get_uwsgi_stats()
    worker_count = len(workers)
    
    if worker_count == 0:
        print("无法从stats服务器获取worker信息，使用默认值")
        worker_count = 5
    else:
        print(f"检测到 {worker_count} 个workers")
        for w in workers:
            print(f"  Worker {w['id']}: PID={w['pid']}, 状态={w['status']}")
    
    # 并发预热
    print(f"\n并发预热所有workers...")
    
    def warmup_request(i):
        try:
            resp = requests.post(
                f"{base_url}/predict",
                json={"warmup": True},
                timeout=60
            )
            return resp.json()
        except Exception as e:
            return {"error": str(e)}
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=worker_count) as executor:
        futures = [executor.submit(warmup_request, i) for i in range(worker_count)]
        results = list(concurrent.futures.as_completed(futures))
    
    print(f"✓ 第一轮并发预热完成")
    
    # 检查所有worker是否都就绪
    time.sleep(2)
    workers_after = get_uwsgi_stats()
    
    if workers_after:
        print(f"\nWorker状态:")
        for w in workers_after:
            print(f"  Worker {w['id']}: 请求数={w['requests']}, 状态={w['status']}")
    
    # 额外几轮预热确保覆盖
    print(f"\n额外预热轮次...")
    for i in range(worker_count * 2):
        try:
            requests.post(f"{base_url}/predict", json={"warmup": True}, timeout=10)
        except:
            pass
        time.sleep(0.1)
    
    print("✓ 预热完成")

def wait_for_service(base_url, max_retry=60):
    for i in range(max_retry):
        try:
            resp = requests.get(f"{base_url}/health", timeout=2)
            if resp.status_code == 200:
                print("✓ 服务已启动")
                return True
        except:
            print(f"等待服务启动... ({i+1}/{max_retry})")
            time.sleep(1)
    raise Exception("服务启动超时")

if __name__ == "__main__":
    warmup_with_monitoring()
```

## 完整的start.sh脚本

```bash
#!/bin/bash
set -e

echo "启动uWSGI (lazy-apps模式)..."
uwsgi --ini /app/uwsgi.ini &
UWSGI_PID=$!

echo "等待uWSGI启动..."
sleep 5

echo "开始预热所有workers..."
python /app/warmup.py

# 检查预热是否成功
if [ $? -eq 0 ]; then
    echo "✓ 预热成功，应用已就绪"
else
    echo "✗ 预热失败"
    exit 1
fi

# 保持uWSGI在前台运行
wait $UWSGI_PID
```

## 关键要点

1. **并发请求**：lazy-apps模式下必须使用并发请求，确保同时触发多个worker
1. **超时设置**：首次请求可能需要很长时间（加载模型），timeout要设置足够大（60s+）
1. **验证覆盖**：通过响应头的worker PID来验证是否所有worker都被预热
1. **多轮预热**：如果worker数量不确定，多发几轮请求确保覆盖

你的模型加载大概需要多长时间？这样我可以帮你调整timeout和预热策略。​​​​​​​​​​​​​​​​
