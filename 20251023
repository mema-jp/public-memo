import snowflake.connector
import pymysql
from typing import List, Tuple, Optional, Dict
import logging
from contextlib import contextmanager
import hashlib
import json
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization

# ログ設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DataTransfer:
    """SnowflakeからMySQLへのデータ転送クラス"""
    
    def __init__(self, snowflake_config: dict, mysql_config: dict):
        """
        初期化設定
        
        :param snowflake_config: Snowflake接続設定
        :param mysql_config: MySQL接続設定
        """
        self.snowflake_config = snowflake_config
        self.mysql_config = mysql_config
    
    @staticmethod
    def load_private_key(private_key_path: str, passphrase: str = None):
        """
        秘密鍵ファイルを読み込む（パスフレーズ対応）
        
        :param private_key_path: 秘密鍵ファイルのパス
        :param passphrase: 秘密鍵のパスフレーズ（オプション）
        :return: 秘密鍵オブジェクト
        """
        try:
            with open(private_key_path, 'rb') as key_file:
                private_key_data = key_file.read()
            
            # パスフレーズをバイト列に変換
            passphrase_bytes = passphrase.encode() if passphrase else None
            
            # 秘密鍵を読み込む
            private_key = serialization.load_pem_private_key(
                private_key_data,
                password=passphrase_bytes,
                backend=default_backend()
            )
            
            # DER形式のバイト列に変換（Snowflakeが要求する形式）
            pkb = private_key.private_bytes(
                encoding=serialization.Encoding.DER,
                format=serialization.PrivateFormat.PKCS8,
                encryption_algorithm=serialization.NoEncryption()
            )
            
            logger.info(f"✓ 秘密鍵を正常に読み込みました: {private_key_path}")
            return pkb
            
        except Exception as e:
            logger.error(f"✗ 秘密鍵の読み込みに失敗: {str(e)}")
            raise
    
    @contextmanager
    def get_snowflake_connection(self):
        """Snowflake接続のコンテキストマネージャー"""
        conn = None
        try:
            # 秘密鍵認証の場合、private_keyパラメータを処理
            config = self.snowflake_config.copy()
            
            if 'private_key_path' in config:
                private_key_path = config.pop('private_key_path')
                passphrase = config.pop('private_key_passphrase', None)
                
                # 秘密鍵を読み込む
                config['private_key'] = self.load_private_key(
                    private_key_path, 
                    passphrase
                )
            
            conn = snowflake.connector.connect(**config)
            logger.info("✓ Snowflakeに接続しました")
            yield conn
        finally:
            if conn:
                conn.close()
                logger.info("✓ Snowflake接続を閉じました")
    
    @contextmanager
    def get_mysql_connection(self):
        """MySQL接続のコンテキストマネージャー"""
        conn = None
        try:
            conn = pymysql.connect(**self.mysql_config)
            yield conn
        finally:
            if conn:
                conn.close()
    
    @staticmethod
    def apply_column_mapping(source_columns: List[str], 
                            source_rows: List[Tuple],
                            column_mapping: Optional[Dict[str, str]] = None) -> Tuple[List[str], List[Tuple]]:
        """
        カラムマッピングを適用してデータを変換
        
        :param source_columns: ソースのカラム名リスト
        :param source_rows: ソースのデータ行リスト
        :param column_mapping: カラムマッピング辞書 {ソースカラム名: ターゲットカラム名}
        :return: (マッピング後のカラム名リスト, マッピング後のデータ行リスト)
        """
        if not column_mapping:
            # マッピングなしの場合、そのまま返す
            return source_columns, source_rows
        
        logger.info("カラムマッピングを適用...")
        logger.info(f"マッピング設定: {column_mapping}")
        
        # ソースカラム名を大文字に変換してインデックスマップを作成
        source_columns_upper = [col.upper() for col in source_columns]
        column_index_map = {col.upper(): idx for idx, col in enumerate(source_columns)}
        
        # マッピング対象のカラムとその順序を決定
        mapped_columns = []
        column_indices = []
        
        for source_col, target_col in column_mapping.items():
            source_col_upper = source_col.upper()
            
            if source_col_upper not in column_index_map:
                raise ValueError(f"ソースカラム '{source_col}' がデータに存在しません。利用可能なカラム: {source_columns}")
            
            mapped_columns.append(target_col)
            column_indices.append(column_index_map[source_col_upper])
        
        logger.info(f"マッピング前のカラム: {source_columns}")
        logger.info(f"マッピング後のカラム: {mapped_columns}")
        
        # データ行を変換（マッピングされたカラムのみ抽出）
        mapped_rows = []
        for row in source_rows:
            mapped_row = tuple(row[idx] for idx in column_indices)
            mapped_rows.append(mapped_row)
        
        logger.info(f"✓ カラムマッピング完了: {len(source_columns)}カラム → {len(mapped_columns)}カラム")
        
        return mapped_columns, mapped_rows
    
    def fetch_data_from_snowflake(self, 
                                   query: str,
                                   params: tuple = None) -> Tuple[List, List]:
        """
        Snowflakeからデータを取得（カスタムクエリ対応）
        
        :param query: SQLクエリ文
        :param params: クエリパラメータ（オプション）
        :return: (カラム名リスト, データ行リスト)
        """
        with self.get_snowflake_connection() as conn:
            cursor = conn.cursor()
            
            logger.info(f"Snowflakeクエリ実行: {query}")
            if params:
                logger.info(f"クエリパラメータ: {params}")
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            # カラム名を取得
            column_names = [desc[0] for desc in cursor.description]
            
            # 全データを取得
            rows = cursor.fetchall()
            logger.info(f"Snowflakeから {len(rows)} 行のデータを取得しました")
            
            cursor.close()
            return column_names, rows
    
    def fetch_data_from_mysql(self, 
                             query: str,
                             params: tuple = None) -> Tuple[List, List]:
        """
        MySQLからデータを取得（検証用）
        
        :param query: SQLクエリ文
        :param params: クエリパラメータ（オプション）
        :return: (カラム名リスト, データ行リスト)
        """
        with self.get_mysql_connection() as conn:
            cursor = conn.cursor()
            
            logger.info(f"MySQLクエリ実行: {query}")
            if params:
                logger.info(f"クエリパラメータ: {params}")
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            # カラム名を取得
            column_names = [desc[0] for desc in cursor.description]
            
            # 全データを取得
            rows = cursor.fetchall()
            logger.info(f"MySQLから {len(rows)} 行のデータを取得しました")
            
            cursor.close()
            return column_names, rows
    
    def backup_mysql_table(self, 
                          database: str, 
                          table: str,
                          backup_suffix: str = '_backup') -> str:
        """
        MySQLテーブルをバックアップ（ロールバック用）
        
        :param database: データベース名
        :param table: テーブル名
        :param backup_suffix: バックアップテーブルの接尾辞
        :return: バックアップテーブル名
        """
        backup_table = f"{table}{backup_suffix}"
        
        with self.get_mysql_connection() as conn:
            cursor = conn.cursor()
            
            try:
                # バックアップテーブルが既に存在するか確認
                cursor.execute(f"""
                    SELECT COUNT(*) FROM information_schema.tables 
                    WHERE table_schema = '{database}' 
                    AND table_name = '{backup_table}'
                """)
                exists = cursor.fetchone()[0] > 0
                
                if exists:
                    logger.info(f"既存のバックアップテーブルを削除: {database}.{backup_table}")
                    cursor.execute(f"DROP TABLE {database}.{backup_table}")
                
                # バックアップを作成（CREATE TABLE ... AS SELECTが高速）
                logger.info(f"バックアップテーブルを作成: {database}.{backup_table}")
                cursor.execute(f"""
                    CREATE TABLE {database}.{backup_table} 
                    AS SELECT * FROM {database}.{table}
                """)
                
                conn.commit()
                
                # バックアップ行数を取得
                cursor.execute(f"SELECT COUNT(*) FROM {database}.{backup_table}")
                backup_count = cursor.fetchone()[0]
                logger.info(f"✓ {backup_count} 行のデータを {database}.{backup_table} にバックアップしました")
                
                cursor.close()
                return backup_table
                
            except Exception as e:
                conn.rollback()
                logger.error(f"✗ テーブルのバックアップに失敗: {str(e)}")
                raise
    
    def restore_from_backup(self, 
                           database: str, 
                           table: str,
                           backup_table: str):
        """
        バックアップからデータを復元
        
        :param database: データベース名
        :param table: テーブル名
        :param backup_table: バックアップテーブル名
        """
        with self.get_mysql_connection() as conn:
            cursor = conn.cursor()
            
            try:
                logger.info(f"バックアップテーブル {backup_table} からデータを復元開始...")
                
                # ターゲットテーブルをクリア（TRUNCATEを使用）
                cursor.execute(f"TRUNCATE TABLE {database}.{table}")
                logger.info(f"✓ テーブル {database}.{table} をクリアしました")
                
                # バックアップテーブルからデータを復元
                cursor.execute(f"""
                    INSERT INTO {database}.{table} 
                    SELECT * FROM {database}.{backup_table}
                """)
                
                conn.commit()
                
                # 復元された行数を取得
                cursor.execute(f"SELECT COUNT(*) FROM {database}.{table}")
                restored_count = cursor.fetchone()[0]
                logger.info(f"✓ バックアップから {restored_count} 行のデータを復元しました")
                
                cursor.close()
                
            except Exception as e:
                conn.rollback()
                logger.error(f"✗ バックアップからの復元に失敗: {str(e)}")
                raise
    
    def delete_backup_table(self, database: str, backup_table: str):
        """
        バックアップテーブルを削除
        
        :param database: データベース名
        :param backup_table: バックアップテーブル名
        """
        with self.get_mysql_connection() as conn:
            cursor = conn.cursor()
            
            try:
                logger.info(f"バックアップテーブルを削除: {database}.{backup_table}")
                cursor.execute(f"DROP TABLE IF EXISTS {database}.{backup_table}")
                conn.commit()
                logger.info(f"✓ バックアップテーブルを削除しました")
                cursor.close()
                
            except Exception as e:
                logger.warning(f"⚠ バックアップテーブルの削除に失敗: {str(e)}")
    
    def truncate_and_insert(self,
                           database: str,
                           table: str,
                           column_names: List[str], 
                           rows: List,
                           batch_size: int = 1000):
        """
        テーブルをTRUNCATEしてデータを挿入
        注意: TRUNCATEはロールバック不可、必ず事前にバックアップを作成すること！
        
        :param database: データベース名
        :param table: テーブル名
        :param column_names: カラム名リスト
        :param rows: データ行リスト
        :param batch_size: バッチ挿入のサイズ
        """
        if not rows:
            logger.warning("挿入するデータがありません")
            return
        
        conn = None
        try:
            # 接続を取得
            conn = pymysql.connect(**self.mysql_config)
            cursor = conn.cursor()
            
            # 1. TRUNCATEでテーブルをクリア（注意: この操作はロールバック不可！）
            logger.info(f"TRUNCATEでテーブルをクリア: {database}.{table}...")
            cursor.execute(f"TRUNCATE TABLE {database}.{table}")
            logger.info(f"✓ テーブルデータをクリアしました")
            
            # 2. INSERT文を構築
            cols_str = ', '.join(column_names)
            placeholders = ', '.join(['%s'] * len(column_names))
            insert_query = f"INSERT INTO {database}.{table} ({cols_str}) VALUES ({placeholders})"
            
            # 3. バッチ挿入（この部分はトランザクション内）
            conn.begin()
            logger.info("データのバッチ挿入を開始...")
            
            total_inserted = 0
            for i in range(0, len(rows), batch_size):
                batch = rows[i:i + batch_size]
                cursor.executemany(insert_query, batch)
                total_inserted += len(batch)
                logger.info(f"挿入済み {total_inserted}/{len(rows)} 行")
            
            # 4. 挿入トランザクションをコミット
            conn.commit()
            logger.info(f"✓ {database}.{table} に {total_inserted} 行のデータを挿入しました")
            
            cursor.close()
            
        except Exception as e:
            # 注意: TRUNCATEは既に実行され、ロールバック不可。INSERT部分のみロールバック可能
            if conn:
                conn.rollback()
                logger.error(f"✗ データ挿入に失敗、INSERT操作をロールバックしました: {str(e)}")
                logger.error(f"⚠ 警告: テーブルは既にTRUNCATEされています、バックアップから復元が必要です！")
            raise
            
        finally:
            if conn:
                conn.close()
    
    def calculate_checksum(self, rows: List, column_names: List[str]) -> str:
        """
        データのチェックサムを計算
        
        :param rows: データ行リスト
        :param column_names: カラム名リスト
        :return: MD5チェックサム
        """
        # データをソート可能な形式に変換
        data_list = []
        for row in rows:
            # 各行を辞書に変換してソート可能にする
            row_dict = {column_names[i]: str(row[i]) if row[i] is not None else 'NULL' 
                       for i in range(len(column_names))}
            data_list.append(row_dict)
        
        # 全カラムでソートして順序を一致させる
        sorted_data = sorted(data_list, key=lambda x: json.dumps(x, sort_keys=True))
        
        # MD5を計算
        data_str = json.dumps(sorted_data, sort_keys=True)
        checksum = hashlib.md5(data_str.encode('utf-8')).hexdigest()
        
        return checksum
    
    def verify_data_consistency(self,
                               source_data: Tuple[List, List],
                               target_data: Tuple[List, List],
                               detailed: bool = True) -> dict:
        """
        ソースとターゲットのデータ整合性を検証
        
        :param source_data: ソースデータ (カラム名, 行データ)
        :param target_data: ターゲットデータ (カラム名, 行データ)
        :param detailed: 詳細比較を行うか
        :return: 検証結果の辞書
        """
        source_columns, source_rows = source_data
        target_columns, target_rows = target_data
        
        result = {
            'is_consistent': True,
            'source_row_count': len(source_rows),
            'target_row_count': len(target_rows),
            'issues': []
        }
        
        # 1. 行数チェック
        logger.info(f"ソースデータ行数: {len(source_rows)}, ターゲットデータ行数: {len(target_rows)}")
        if len(source_rows) != len(target_rows):
            result['is_consistent'] = False
            result['issues'].append(f"行数不一致: ソース{len(source_rows)}行, ターゲット{len(target_rows)}行")
        
        # 2. カラム名チェック（マッピング後なので同じはず）
        logger.info(f"ソースカラム名: {source_columns}")
        logger.info(f"ターゲットカラム名: {target_columns}")
        
        if source_columns != target_columns:
            result['is_consistent'] = False
            result['issues'].append(f"カラム名不一致: ソース{source_columns}, ターゲット{target_columns}")
        
        # 3. チェックサム計算
        if len(source_rows) > 0 and len(target_rows) > 0:
            source_checksum = self.calculate_checksum(source_rows, source_columns)
            target_checksum = self.calculate_checksum(target_rows, target_columns)
            
            result['source_checksum'] = source_checksum
            result['target_checksum'] = target_checksum
            
            logger.info(f"ソースデータチェックサム: {source_checksum}")
            logger.info(f"ターゲットデータチェックサム: {target_checksum}")
            
            if source_checksum != target_checksum:
                result['is_consistent'] = False
                result['issues'].append("データ内容のチェックサムが不一致")
                
                # 詳細比較が必要な場合
                if detailed and len(source_rows) == len(target_rows):
                    logger.info("詳細なデータ比較を実行中...")
                    diff_count = 0
                    for i, (src_row, tgt_row) in enumerate(zip(source_rows, target_rows)):
                        if src_row != tgt_row:
                            diff_count += 1
                            if diff_count <= 5:  # 最初の5件の差異のみ表示
                                result['issues'].append(
                                    f"第{i+1}行のデータが不一致:\n  ソース: {src_row}\n  ターゲット: {tgt_row}"
                                )
                    
                    if diff_count > 5:
                        result['issues'].append(f"...他に{diff_count - 5}行のデータが不一致")
        
        return result
    
    def transfer(self,
                source_query: str,
                target_database: str,
                target_table: str,
                column_mapping: Optional[Dict[str, str]] = None,
                source_query_params: tuple = None,
                batch_size: int = 1000,
                verify: bool = True,
                verify_query: Optional[str] = None,
                use_backup: bool = True,
                keep_backup_on_success: bool = False,
                rollback_on_verify_fail: bool = True) -> dict:
        """
        完全なデータ転送フローを実行（TRUNCATE + バックアップ保護 + カラムマッピング）
        
        重要: TRUNCATEはロールバック不可のため、本メソッドはバックアップ作成を強制します！
        
        :param source_query: ソースデータクエリSQL（Snowflake）
        :param target_database: ターゲットデータベース（MySQL）
        :param target_table: ターゲットテーブル名（MySQL）
        :param column_mapping: カラムマッピング {ソースカラム名: ターゲットカラム名}
        :param source_query_params: ソースクエリパラメータ
        :param batch_size: バッチ挿入サイズ
        :param verify: データ整合性を検証するか
        :param verify_query: カスタム検証クエリ
        :param use_backup: バックアップを使用するか（TRUNCATEではTrue推奨）
        :param keep_backup_on_success: 成功時にバックアップテーブルを保持するか
        :param rollback_on_verify_fail: 検証失敗時にロールバックするか
        :return: 転送結果と検証結果を含む辞書
        """
        transfer_result = {
            'success': False,
            'rows_transferred': 0,
            'verification': None,
            'backup_created': False,
            'rollback_performed': False
        }
        
        backup_table = None
        
        try:
            logger.info("=" * 80)
            logger.info(f"{target_database}.{target_table} へのデータ転送を開始")
            logger.info(f"転送モード: TRUNCATE（ロールバック不可） + バックアップ保護")
            if column_mapping:
                logger.info(f"カラムマッピング有効: {len(column_mapping)} カラム")
            logger.info("=" * 80)
            
            # 安全性チェック: TRUNCATEを使用する場合、バックアップは必須
            if not use_backup:
                logger.warning("⚠ 警告: TRUNCATEはロールバック不可、バックアップ保護を強制的に有効化します！")
                use_backup = True
            
            # 1. Snowflakeからデータを取得
            logger.info("ステップ1: Snowflakeからデータを取得...")
            source_columns, source_rows = self.fetch_data_from_snowflake(
                source_query, source_query_params
            )
            
            if not source_rows:
                logger.warning("ソースクエリがデータを返しませんでした、転送をスキップします")
                return transfer_result
            
            # 1.5. カラムマッピングを適用
            if column_mapping:
                logger.info("ステップ1.5: カラムマッピングを適用...")
                mapped_columns, mapped_rows = self.apply_column_mapping(
                    source_columns, source_rows, column_mapping
                )
            else:
                logger.info("カラムマッピングなし、元のカラム名を使用")
                mapped_columns = source_columns
                mapped_rows = source_rows
            
            # 2. バックアップを作成（TRUNCATEを使用する場合は必須！）
            logger.info("ステップ2: データをバックアップ...")
            backup_table = self.backup_mysql_table(target_database, target_table)
            transfer_result['backup_created'] = True
            transfer_result['backup_table'] = backup_table
            
            # 3. TRUNCATEしてデータを挿入
            logger.info("ステップ3: テーブルをTRUNCATEしてデータを挿入...")
            self.truncate_and_insert(
                target_database, target_table, mapped_columns, mapped_rows, batch_size
            )
            
            transfer_result['success'] = True
            transfer_result['rows_transferred'] = len(mapped_rows)
            
            # 4. データ整合性検証
            if verify:
                logger.info("=" * 80)
                logger.info("ステップ4: データ整合性を検証...")
                logger.info("=" * 80)
                
                # ターゲットクエリを構築
                if verify_query is None:
                    cols_str = ', '.join(mapped_columns)
                    verify_query = f"SELECT {cols_str} FROM {target_database}.{target_table}"
                
                # ターゲットデータを取得
                target_columns, target_rows = self.fetch_data_from_mysql(verify_query)
                
                # 検証を実行（マッピング後のデータで比較）
                verification_result = self.verify_data_consistency(
                    (mapped_columns, mapped_rows),
                    (target_columns, target_rows)
                )
                
                transfer_result['verification'] = verification_result
                
                # 検証結果を出力
                logger.info("=" * 80)
                if verification_result['is_consistent']:
                    logger.info("✓ データ整合性検証が成功しました！")
                    
                    # 検証成功、設定に応じてバックアップテーブルを削除
                    if not keep_backup_on_success:
                        self.delete_backup_table(target_database, backup_table)
                    else:
                        logger.info(f"ℹ バックアップテーブルを保持: {target_database}.{backup_table}")
                        
                else:
                    logger.error("✗ データ整合性検証が失敗しました！")
                    for issue in verification_result['issues']:
                        logger.error(f"  - {issue}")
                    
                    # 検証失敗、ロールバックするか
                    if rollback_on_verify_fail:
                        logger.warning("=" * 80)
                        logger.warning("検証失敗、ロールバックを実行...")
                        logger.warning("=" * 80)
                        
                        self.restore_from_backup(target_database, target_table, backup_table)
                        transfer_result['rollback_performed'] = True
                        transfer_result['success'] = False
                        
                        logger.info("✓ 元のデータにロールバックしました")
                        
                        # ロールバック後、検査用にバックアップテーブルを保持
                        logger.info(f"ℹ 検査用にバックアップテーブルを保持: {target_database}.{backup_table}")
                    else:
                        logger.warning("検証は失敗しましたがロールバックは無効、データは現在の状態を維持します")
                        logger.info(f"ℹ 復元用にバックアップテーブルを保持: {target_database}.{backup_table}")
                        
                logger.info("=" * 80)
            else:
                # 検証なし、設定に応じてバックアップテーブルを削除
                if not keep_backup_on_success:
                    self.delete_backup_table(target_database, backup_table)
                else:
                    logger.info(f"ℹ バックアップテーブルを保持: {target_database}.{backup_table}")
            
            logger.info("=" * 80)
            logger.info("データ転送フローが完了しました！")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"✗ データ転送中にエラーが発生しました: {str(e)}", exc_info=True)
            transfer_result['error'] = str(e)
            
            # 例外発生、バックアップからロールバック
            if backup_table:
                try:
                    logger.warning("=" * 80)
                    logger.warning("例外発生、バックアップからのロールバックを試行...")
                    logger.warning("=" * 80)
                    
                    self.restore_from_backup(target_database, target_table, backup_table)
                    transfer_result['rollback_performed'] = True
                    
                    logger.info("✓ 元のデータにロールバックしました")
                    logger.info(f"ℹ 検査用にバックアップテーブルを保持: {target_database}.{backup_table}")
                    
                except Exception as rollback_error:
                    logger.error(f"✗ ロールバックに失敗: {str(rollback_error)}")
                    transfer_result['rollback_error'] = str(rollback_error)
                    logger.error(f"⚠ 緊急: 元のデータはバックアップテーブル {target_database}.{backup_table} にあります！")
            
            raise
        
        return transfer_result


def main():
    """メイン関数の例"""
    
    # Snowflake設定
    snowflake_config = {
        'user': 'your_username',
        'account': 'abc12345.us-east-1',
        'warehouse': 'your_warehouse',
        'private_key_path': '/path/to/rsa_key.p8',
        'private_key_passphrase': 'your_passphrase'
    }
    
    # MySQL設定
    mysql_config = {
        'host': 'localhost',
        'port': 3306,
        'user': 'your_username',
        'password': 'your_password',
        'charset': 'utf8mb4',
        'autocommit': False
    }
    
    # 転送オブジェクトを作成
    transfer = DataTransfer(snowflake_config, mysql_config)
    
    # ========== 例1: カラム名が完全に異なる場合 ==========
    print("\n" + "="*80)
    print("例1: カラムマッピング - 完全に異なるカラム名")
    print("="*80)
    
    # Snowflakeのカラム: ORDER_ID, CUSTOMER_NAME, ORDER_DATE, TOTAL_AMOUNT
    # MySQLのカラム: id, customer, created_at, total
    column_mapping_1 = {
        'ORDER_ID': 'id',
        'CUSTOMER_NAME': 'customer',
        'ORDER_DATE': 'created_at',
        'TOTAL_AMOUNT': 'total'
    }
    
    result1 = transfer.transfer(
        source_query='SELECT ORDER_ID, CUSTOMER_NAME, ORDER_DATE, TOTAL_AMOUNT FROM MYDB.PUBLIC.ORDERS',
        target_database='mydb',
        target_table='orders',
        column_mapping=column_mapping_1,
        batch_size=1000,
        verify=True
    )
    
    print(f"\n転送結果:")
    print(f"  - 成功: {result1['success']}")
    print(f"  - 転送行数: {result1['rows_transferred']}")
    
    # ========== 例2: 一部のカラムのみ転送（カラム選択） ==========
    print("\n" + "="*80)
    print("例2: 一部カラムのみ転送")
    print("="*80)
    
    # Snowflakeに10カラムあるが、MySQLには5カラムのみ転送
    column_mapping_2 = {
        'USER_ID': 'user_id',
        'USER_NAME': 'name',
        'EMAIL_ADDRESS': 'email',
        'PHONE_NUMBER': 'phone',
        'REGISTRATION_DATE': 'registered_at'
        # 他のカラムは無視される
    }
    
    result2 = transfer.transfer(
        source_query='SELECT * FROM MYDB.PUBLIC.USERS',
        target_database='mydb',
        target_table='users',
        column_mapping=column_mapping_2,
        batch_size=1000,
        verify=True
    )
    
    # ========== 例3: カラムの順序を変更 ==========
    print("\n" + "="*80)
    print("例3: カラムの順序を変更")
    print("="*80)
    
    # Snowflake: ID, NAME, AGE, CITY
    # MySQL: city, age, name, id (順序が異なる)
    column_mapping_3 = {
        'ID': 'id',
        'NAME': 'name',
        'AGE': 'age',
        'CITY': 'city'
    }
    
    result3 = transfer.transfer(
        source_query='SELECT ID, NAME, AGE, CITY FROM MYDB.PUBLIC.EMPLOYEES',
        target_database='mydb',
        target_table='employees',
        column_mapping=column_mapping_3,
        batch_size=1000,
        verify=True
    )
    
    # ========== 例4: 大文字小文字が異なる ==========
    print("\n" + "="*80)
    print("例4: 大文字小文字の違い")
    print("="*80)
    
    # Snowflake: PRODUCT_ID, PRODUCT_NAME (大文字)
    # MySQL: product_id, product_name (小文字)
    column_mapping_4 = {
        'PRODUCT_ID': 'product_id',
        'PRODUCT_NAME': 'product_name',
        'PRICE': 'price',
        'STOCK_QUANTITY': 'stock'
    }
    
    result4 = transfer.transfer(
        source_query='SELECT PRODUCT_ID, PRODUCT_NAME, PRICE, STOCK_QUANTITY FROM MYDB.PUBLIC.PRODUCTS',
        target_database='mydb',
        target_table='products',
        column_mapping=column_mapping_4,
        verify=True
    )
    
    # ========== 例5: カラムマッピングなし（同じカラム名） ==========
    print("\n" + "="*80)
    print("例5: カラムマッピングなし（カラム名が同じ場合）")
    print("="*80)
    
    result5 = transfer.transfer(
        source_query='SELECT id, name, email FROM MYDB.PUBLIC.CUSTOMERS',
        target_database='mydb',
        target_table='customers',
        column_mapping=None,  # マッピングなし
        verify=True
    )
    
    # ========== 例6: 設定ファイルからマッピングを読み込む ==========
    print("\n" + "="*80)
    print("例6: JSONファイルからマッピングを読み込む")
    print("="*80)
    
    # mapping.json ファイルの例:
    # {
    #   "ORDER_ID": "id",
    #   "CUSTOMER_NAME": "customer",
    #   "ORDER_DATE": "created_at"
    # }
    
    import json
    
    # JSONファイルからマッピングを読み込む
    # with open('mapping.json', 'r', encoding='utf-8') as f:
    #     column_mapping_from_file = json.load(f)
    
    # 例として直接定義
    column_mapping_from_file = {
        "ORDER_ID": "id",
        "CUSTOMER_NAME": "customer",
        "ORDER_DATE": "created_at",
        "TOTAL_AMOUNT": "total"
    }
    
    result6 = transfer.transfer(
        source_query='SELECT ORDER_ID, CUSTOMER_NAME, ORDER_DATE, TOTAL_AMOUNT FROM MYDB.PUBLIC.ORDERS',
        target_database='mydb',
        target_table='orders_new',
        column_mapping=column_mapping_from_file,
        verify=True
    )


if __name__ == '__main__':
    main()

pip install snowflake-connector-python pymysql
